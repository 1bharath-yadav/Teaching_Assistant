# ******************** Teaching Assistant Configuration ********************#
# This file manages all model configurations and project variables

# ******************** model providers ********************#

# OpenAI Configuration
openai:
  # For official OpenAI API
  api_key: "${OPENAI_API_KEY:-your-openai-api-key-here}"
  base_url: "https://api.openai.com/v1"
  organization_id: "${OPENAI_ORG_ID:-}"
  models:
    - "gpt-4"
    - "gpt-4-turbo"
    - "gpt-4o"
    - "gpt-4o-mini" 
    - "gpt-3.5-turbo"
  default_model: "gpt-4o-mini"
  max_tokens: 4096
  temperature: 0.7

# Ollama Configuration (Local LLM Server)
ollama:
  base_url: "${OLLAMA_BASE_URL:-http://localhost:11434/v1}"
  api_key: "${OLLAMA_API_KEY:-ollama}"  # Ollama typically doesn't require auth
  models:
    - "llama3.2:1b"
    - "gemma3:12b-it-qat"
    - "deepseek-r1:latest"
    - "nomic-embed-text:latest"
    - "gemma3:4b"
  default_model: "gemma3:4b"
  max_tokens: 4096
  temperature: 0.4
  # Embedding models for local RAG
  embedding_models:
    - "nomic-embed-text"
    - "mxbai-embed-large"
    - "all-minilm"

# Azure OpenAI Configuration
azure:
  enabled: false
  api_key: "${AZURE_API_KEY:-}"
  base_url: "${AZURE_URL:-}"
  api_version: "${AZURE_API_VERSION:-2023-12-01-preview}"
  deployment_name: "${AZURE_DEPLOYMENT_NAME:-}"
  max_tokens: 4096
  temperature: 0.7

# ******************** other AI providers ********************#

# Anthropic (Claude)
anthropic:
  enabled: false
  api_key: "${ANTHROPIC_API_KEY:-}"
  base_url: "${ANTHROPIC_URL:-https://api.anthropic.com}"
  api_version: "${ANTHROPIC_API_VERSION:-2023-06-01}"
  models:
    - "claude-3-5-sonnet-20241022"
    - "claude-3-opus-20240229"
    - "claude-3-haiku-20240307"
  max_tokens: 4096
  temperature: 0.7

# Google (Gemini)
google:
  enabled: false
  api_key: "${GOOGLE_API_KEY:-}"
  base_url: "${GOOGLE_URL:-https://generativelanguage.googleapis.com}"
  models:
    - "gemini-1.5-pro"
    - "gemini-1.5-flash"
  max_tokens: 4096
  temperature: 0.7

# === SEARCH & RAG CONFIGURATION ===

# Typesense Search Engine
typesense:
  api_key: "${TYPESENSE_API_KEY:-xyz}"
  nodes:
    - host: "${TYPESENSE_HOST:-localhost}"
      port: "${TYPESENSE_PORT:-8108}"
      protocol: "${TYPESENSE_PROTOCOL:-http}"
  connection_timeout: 10
  collections:
    discourse:
      name: "discourse_posts"
      enable_hybrid_search: true
    documents:
      name: "documents"
      enable_hybrid_search: true

# Embedding Configuration
embeddings:
  # Primary embedding service
  provider: "ollama"  # Options: "openai", "ollama", "azure"
  
  # OpenAI embeddings
  openai:
    model: "text-embedding-3-small"
    dimensions: 1536
    # Available models with dimensions:
    # text-embedding-3-small: 1536
    # text-embedding-3-large: 3072
    # text-embedding-ada-002: 1536
    
  # Ollama embeddings (local)
  ollama:
    model: "nomic-embed-text"
    dimensions: 768
    # Available models with dimensions:
    # nomic-embed-text: 768
    # mxbai-embed-large: 1024
    # all-minilm: 384
    # snowflake-arctic-embed: 1024
    
  # Azure embeddings
  azure:
    model: "text-embedding-ada-002"
    dimensions: 1536

# Model-specific embedding dimensions (for reference)
model_dimensions:
  # OpenAI models
  "text-embedding-3-small": 1536
  "text-embedding-3-large": 3072
  "text-embedding-ada-002": 1536
  # Ollama models
  "nomic-embed-text": 768
  "mxbai-embed-large": 1024
  "all-minilm": 384
  "snowflake-arctic-embed": 1024

# ******************** project configuration ********************#

# Application Settings
app:
  name: "Teaching Assistant"
  version: "1.0.0"
  debug: true
  log_level: "INFO"
  
# Frontend Configuration
frontend:
  port: 3000
  host: "localhost"
  api_base_path: "/api"
  
# Backend API Configuration  
api:
  port: 8000
  host: "localhost"
  workers: 1
  cors_origins:
    - "http://localhost:3000"
    - "http://127.0.0.1:3000"

# ******************** security configuration ********************#
security:
  access_codes: []  # Add access codes here if needed
  enable_api_key_auth: false
  allowed_origins:
    - "http://localhost:3000"
    - "http://127.0.0.1:3000"

# ******************** processing configuration ********************#

# Document Processing
document_processing:
  chunk_size: 3000
  chunk_overlap: 200
  min_chunk_length: 50
  max_chunk_length: 8000
  enable_content_validation: true
  supported_formats:
    - "pdf"
    - "txt" 
    - "md"
    - "html"
    - "docx"

# Discourse Processing
discourse:
  input_file: "data/discourse/scraped_posts.json"
  output_file: "data/discourse/processed_posts.json"
  batch_size: 100
  enable_html_cleaning: true
  enable_embeddings: true
  enable_content_validation: true
  
# Chapter Processing
chapters:
  repository_url: "https://github.com/sanand0/tools-in-data-science-public.git"
  local_path: "data/chapters/tools-in-data-science-public"
  modules:
    - "development_tools"
    - "deployment_tools" 
    - "large_language_models"
    - "data_sourcing"
    - "data_preparation"
    - "data_analysis"
    - "data_visualization"
    - "project-1"
    - "project-2"
    - "misc"
  chunking:
    intelligent_splitting: true
    preserve_headers: true
    extract_metadata: true
    
# RAG Pipeline Settings
rag_pipeline:
  enable_batch_processing: true
  max_concurrent_requests: 10
  retry_attempts: 3
  retry_delay: 1.0
  enable_progress_tracking: true
  save_intermediate_results: true

# ******************** LLM hyperparameters ********************#
# Task-specific LLM parameters for different use cases

llm_hyperparameters:
  # Answer generation parameters
  answer_generation:
    temperature: 0.7        # Creative but controlled for Q&A
    max_tokens: 1500       # Sufficient for comprehensive answers
    
  # Classification parameters  
  classification:
    temperature: 0.1        # Low temperature for consistent classification
    max_tokens: 100        # Short response expected for classification
    
  # General chat parameters
  general_chat:
    temperature: 0.7        # Balanced creativity
    max_tokens: 2000       # Standard response length
    
  # Tool calling parameters
  tool_calling:
    temperature: 0.1        # Precise for function calls
    max_tokens: 200        # Brief structured responses

# === HYBRID SEARCH CONFIGURATION ===
# Configuration for the Teaching Assistant API hybrid search

hybrid_search:
  # Search Strategy Selection
  # Options: "classification", "enhanced", "unified"
  # - classification: Original LLM-based approach (slow but targeted)
  # - enhanced: Keyword-based routing with fallback (fast, good coverage)
  # - unified: Direct unified knowledge base search (fastest, comprehensive)
  search_strategy: "unified"  # Choose your preferred strategy
  
  # Search parameters
  alpha: 0.5                    # Hybrid search balance (0.0 = pure keyword, 1.0 = pure semantic)
  top_k: 4                     # Number of results to return
  max_context_length: 1000     # Maximum context length for answer generation
  num_typos: 4                  # Number of typos allowed in search
  
  # Strategy-specific configurations
  strategies:
    # Classification-based search (original approach)
    classification:
      enabled: true
      use_tool_calling: true    # Use Ollama tool calling for classification
      max_classification_time: 10  # Max seconds for classification
      fallback_collections:    # Collections to use if classification fails
        - "discourse_posts_optimized"
        - "chapters_misc"
        - "unified_knowledge_base"
    
    # Enhanced keyword-based search
    enhanced:
      enabled: true
      priority_collections:    # Always searched first
        - "discourse_posts_optimized"
        - "chapters_misc"
      fallback_collection: "unified_knowledge_base"
      use_fallback: true       # Enable fallback if insufficient results
      min_results_for_fallback: 3  # Trigger fallback if fewer results
      
      # Keyword-to-collection mapping for fast routing
      keyword_collections:
        # Data sourcing
        "scraping": ["chapters_data_sourcing"]
        "scrape": ["chapters_data_sourcing"]
        "api": ["chapters_data_sourcing"]
        "data collection": ["chapters_data_sourcing"]
        "web scraping": ["chapters_data_sourcing"]
        # LLM and AI
        "llm": ["chapters_large_language_models"]
        "language model": ["chapters_large_language_models"]
        "embedding": ["chapters_large_language_models"]
        "prompt": ["chapters_large_language_models"]
        "ai": ["chapters_large_language_models"]
        # Development tools
        "docker": ["chapters_development_tools", "chapters_deployment_tools"]
        "git": ["chapters_development_tools"]
        "python": ["chapters_development_tools"]
        "development": ["chapters_development_tools"]
        # Projects
        "project": ["chapters_project-1", "chapters_project-2"]
        "assignment": ["chapters_project-1", "chapters_project-2"]
    
    # Unified knowledge base search (fastest, most comprehensive)
    unified:
      enabled: true
      primary_collection: "unified_knowledge_base"  # Contains all 1530 documents
      max_results: 10          # Get more results for better coverage
      content_type_boosting:   # Boost specific content types
        discourse: 1.8         # Student Q&A gets highest priority
        misc: 1.5             # Live sessions get high priority
        technical: 1.2        # Technical content gets moderate boost
        general: 1.0          # General content baseline
      enable_content_filtering: true  # Enable filtering by content type
  
  # Default collections to search (legacy support)
  default_collections:
    - "chapters_misc"
    - "discourse_posts"
  
  # Available collections for classification (legacy support)
  available_collections:
    - "chapters_data_sourcing"
    - "chapters_data_preparation" 
    - "chapters_data_analysis"
    - "chapters_data_visualization"
    - "chapters_large_language_models"
    - "chapters_development_tools"
    - "chapters_deployment_tools"
    - "chapters_project-1"
    - "chapters_project-2"
    - "chapters_misc"
    - "discourse_posts_optimized"
    - "unified_knowledge_base"
  
  # Answer generation settings
  answer_generation:
    enable_streaming: true        # Enable streaming responses
    enable_link_extraction: true # Extract links from content
    max_sources: 10              # Maximum number of sources to include
    deduplicate_content: true    # Remove duplicate content
    include_source_info: true    # Include source collection info
  
  # System prompts (can be customized)
  prompts:
    classification_system: "Your task is to classify the user's question into one or more relevant collections of the Data Science course."
    assistant_system: |
      You are a helpful and professional teaching assistant for the 'Tools for Data Science' (TDS) course at IIT Madras. Your role is to assist students by answering their questions using the provided course content.

      Guidelines:
      1. Answer based strictly on the course content provided. Do not make up information.
      2. If the course content does not fully address the question, state this clearly and explain what is available.
      3. Maintain a clear, concise, and respectful tone. Avoid casual or filler phrases like "Letâ€™s tackle..." or "What the context says."
      4. Structure your response with headings and bullet points if helpful.
      5. Where applicable, refer to specific module names or resource links from the course content.
      6. Encourage students to ask follow-up questions if needed.
    link_extraction_system: "Extract any URLs and links from the given content. Return as JSON array."
  
  # Fallback settings
  fallback:
    enable_keyword_search: true   # Enable keyword-only fallback
    min_results_threshold: 1      # Minimum results before fallback
    error_messages:
      no_results: "I couldn't find relevant information for your question. Please try rephrasing or asking a more specific question."
      search_error: "I encountered an error while processing your question. Please try again."
      generation_error: "I found relevant information but encountered an issue generating the response. Please try again."
  
# ******************** quality control ********************#
quality_control:
  min_content_length: 50
  max_content_length: 8000
  min_alpha_ratio: 0.3
  skip_header_only_chunks: true
  validate_embeddings: true
  
# ******************** environment overrides ********************#
# These can be overridden by environment variables
# Use format: SECTION__SUBSECTION__KEY (double underscore)
# Example: OPENAI__API_KEY, OLLAMA__BASE_URL, TYPESENSE__API_KEY

# ******************** default provider selection ********************#
# Which provider to use by default for different tasks
defaults:
  chat_provider: "ollama"  # Options: "openai", "ollama", "azure", "anthropic", "google"
  embedding_provider: "ollama"  # Options: "openai", "ollama", "azure"
  search_provider: "typesense"  # Options: "typesense"
  tool_calling_model: "llama3.2:1b"  # Specific model for tool calling/classification
