# ******************** Teaching Assistant Configuration ********************#
# This file manages all model configurations and project variables

# ******************** model providers ********************#

# OpenAI Configuration
openai:
  # For official OpenAI API
  api_key: "${OPENAI_API_KEY:-your-openai-api-key-here}"
  base_url: "https://api.openai.com/v1"
  organization_id: "${OPENAI_ORG_ID:-}"
  models:
    - "gpt-4"
    - "gpt-4-turbo"
    - "gpt-4o"
    - "gpt-4o-mini" 
    - "gpt-3.5-turbo"
  default_model: "gpt-4o-mini"
  max_tokens: 4096
  temperature: 0.7

# Ollama Configuration (Local LLM Server)
ollama:
  base_url: "${OLLAMA_BASE_URL:-http://localhost:11434/v1}"
  api_key: "${OLLAMA_API_KEY:-ollama}"  # Ollama typically doesn't require auth
  models:
    - "llama3.2:1b"
    - "gemma3:12b-it-qat"
    - "deepseek-r1:latest"
    - "nomic-embed-text:latest"
    - "gemma3:4b"
  default_model: "gemma3:4b"
  max_tokens: 4096
  temperature: 0.4
  # Embedding models for local RAG
  embedding_models:
    - "nomic-embed-text"
    - "mxbai-embed-large"
    - "all-minilm"

# Azure OpenAI Configuration
azure:
  enabled: false
  api_key: "${AZURE_API_KEY:-}"
  base_url: "${AZURE_URL:-}"
  api_version: "${AZURE_API_VERSION:-2023-12-01-preview}"
  deployment_name: "${AZURE_DEPLOYMENT_NAME:-}"

# ******************** other AI providers ********************#

# Anthropic (Claude)
anthropic:
  enabled: false
  api_key: "${ANTHROPIC_API_KEY:-}"
  base_url: "${ANTHROPIC_URL:-https://api.anthropic.com}"
  api_version: "${ANTHROPIC_API_VERSION:-2023-06-01}"
  models:
    - "claude-3-5-sonnet-20241022"
    - "claude-3-opus-20240229"
    - "claude-3-haiku-20240307"

# Google (Gemini)
google:
  enabled: false
  api_key: "${GOOGLE_API_KEY:-}"
  base_url: "${GOOGLE_URL:-https://generativelanguage.googleapis.com}"
  models:
    - "gemini-1.5-pro"
    - "gemini-1.5-flash"

# === SEARCH & RAG CONFIGURATION ===

# Typesense Search Engine
typesense:
  api_key: "${TYPESENSE_API_KEY:-conscious-field}"
  nodes:
    - host: "${TYPESENSE_HOST:-localhost}"
      port: "${TYPESENSE_PORT:-8108}"
      protocol: "${TYPESENSE_PROTOCOL:-http}"
  connection_timeout: 10
  collections:
    discourse:
      name: "discourse_posts"
      enable_hybrid_search: true
    documents:
      name: "documents"
      enable_hybrid_search: true

# Embedding Configuration
embeddings:
  # Primary embedding service
  provider: "ollama"  # Options: "openai", "ollama", "azure"
  
  # OpenAI embeddings
  openai:
    model: "text-embedding-3-small"
    dimensions: 1536
    # Available models with dimensions:
    # text-embedding-3-small: 1536
    # text-embedding-3-large: 3072
    # text-embedding-ada-002: 1536
    
  # Ollama embeddings (local)
  ollama:
    model: "nomic-embed-text"
    dimensions: 768
    # Available models with dimensions:
    # nomic-embed-text: 768
    # mxbai-embed-large: 1024
    # all-minilm: 384
    # snowflake-arctic-embed: 1024
    
  # Azure embeddings
  azure:
    model: "text-embedding-ada-002"
    dimensions: 1536

# Model-specific embedding dimensions (for reference)
model_dimensions:
  # OpenAI models
  "text-embedding-3-small": 1536
  "text-embedding-3-large": 3072
  "text-embedding-ada-002": 1536
  # Ollama models
  "nomic-embed-text": 768
  "mxbai-embed-large": 1024
  "all-minilm": 384
  "snowflake-arctic-embed": 1024

# ******************** project configuration ********************#

# Application Settings
app:
  name: "Teaching Assistant"
  version: "1.0.0"
  debug: true
  log_level: "INFO"
  
# Frontend Configuration
frontend:
  port: 3000
  host: "localhost"
  api_base_path: "/api"
  
# Backend API Configuration  
api:
  port: 8000
  host: "localhost"
  workers: 1
  cors_origins:
    - "http://localhost:3000"
    - "http://127.0.0.1:3000"

# ******************** security configuration ********************#
security:
  access_codes: []  # Add access codes here if needed
  enable_api_key_auth: false
  allowed_origins:
    - "http://localhost:3000"
    - "http://127.0.0.1:3000"

# ******************** processing configuration ********************#

# Document Processing
document_processing:
  chunk_size: 1000
  chunk_overlap: 200
  min_chunk_length: 50
  max_chunk_length: 8000
  enable_content_validation: true
  supported_formats:
    - "pdf"
    - "txt" 
    - "md"
    - "html"
    - "docx"

# Discourse Processing
discourse:
  input_file: "data/discourse/scraped_posts.json"
  output_file: "data/discourse/processed_posts.json"
  batch_size: 100
  enable_html_cleaning: true
  enable_embeddings: true
  enable_content_validation: true
  
# Chapter Processing
chapters:
  repository_url: "https://github.com/sanand0/tools-in-data-science-public.git"
  local_path: "data/chapters/tools-in-data-science-public"
  modules:
    - "development_tools"
    - "deployment_tools" 
    - "large_language_models"
    - "data_sourcing"
    - "data_preparation"
    - "data_analysis"
    - "data_visualization"
    - "project-1"
    - "project-2"
    - "misc"
  chunking:
    intelligent_splitting: true
    preserve_headers: true
    extract_metadata: true
    
# RAG Pipeline Settings
rag_pipeline:
  enable_batch_processing: true
  max_concurrent_requests: 10
  retry_attempts: 3
  retry_delay: 1.0
  enable_progress_tracking: true
  save_intermediate_results: true

# === HYBRID SEARCH CONFIGURATION ===
# Configuration for the Teaching Assistant API hybrid search

hybrid_search:
  # Search parameters
  alpha: 0.7                    # Hybrid search balance (0.0 = pure keyword, 1.0 = pure semantic)
  top_k: 5                     # Number of results to return
  max_context_length: 50000     # Maximum context length for answer generation
  num_typos: 4                  # Number of typos allowed in search
  
  # Default collections to search (always included)
  default_collections:
    - "chapters_misc"
    - "discourse_posts"
  
  # Available collections for classification
  available_collections:
    - "chapters_data_sourcing"
    - "chapters_data_preparation" 
    - "chapters_data_analysis"
    - "chapters_data_visualization"
    - "chapters_large_language_models"
    - "chapters_development_tools"
    - "chapters_deployment_tools"
    - "chapters_project-1"
    - "chapters_project-2"
    - "chapters_misc"
    - "discourse_posts_optimized"
    - "unified_knowledge_base"
  
  # Answer generation settings
  answer_generation:
    enable_streaming: true        # Enable streaming responses
    enable_link_extraction: true # Extract links from content
    max_sources: 10              # Maximum number of sources to include
    deduplicate_content: true    # Remove duplicate content
    include_source_info: true    # Include source collection info
  
  # System prompts (can be customized)
  prompts:
    classification_system: "Your task is to classify the user's question into one or more relevant collections of the Data Science course."
    assistant_system: |
      You are a helpful teaching assistant for a Tools in datascience(TDS) course in IIT Madras. Your task is to answer student questions using the provided course content.

      Guidelines:
      1. Provide clear, accurate, and helpful answers based on the course content
      2. If the content doesn't fully answer the question, say so and provide what information is available
      3. Be encouraging and supportive in your tone
      4. provide relevant links or resources in the content.
      5. Structure your answer in a logical, easy-to-follow manner
    link_extraction_system: "Extract any URLs and links from the given content. Return as JSON array."
  
  # Fallback settings
  fallback:
    enable_keyword_search: true   # Enable keyword-only fallback
    min_results_threshold: 1      # Minimum results before fallback
    error_messages:
      no_results: "I couldn't find relevant information for your question. Please try rephrasing or asking a more specific question."
      search_error: "I encountered an error while processing your question. Please try again."
      generation_error: "I found relevant information but encountered an issue generating the response. Please try again."
  
# ******************** quality control ********************#
quality_control:
  min_content_length: 50
  max_content_length: 8000
  min_alpha_ratio: 0.3
  skip_header_only_chunks: true
  validate_embeddings: true
  
# ******************** environment overrides ********************#
# These can be overridden by environment variables
# Use format: SECTION__SUBSECTION__KEY (double underscore)
# Example: OPENAI__API_KEY, OLLAMA__BASE_URL, TYPESENSE__API_KEY

# ******************** default provider selection ********************#
# Which provider to use by default for different tasks
defaults:
  chat_provider: "ollama"  # Options: "openai", "ollama", "azure", "anthropic", "google"
  embedding_provider: "ollama"  # Options: "openai", "ollama", "azure"
  search_provider: "typesense"  # Options: "typesense"
  tool_calling_model: "llama3.2:1b"  # Specific model for tool calling/classification
